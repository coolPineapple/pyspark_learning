{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data in PySpark\n",
    "\n",
    "In the real world, most datasets you work with will be incomplete, which means you will have missing data. You have 2 basic options for filling in missing data (you will personally have to make the decision for what is the right approach):\n",
    "\n",
    "1. Drop the missing data points (including the possbily the entire row)\n",
    "2. Fill them in with some other value (like the average).\n",
    "\n",
    "There are also two different types of missing data to be aware of:\n",
    "\n",
    "1. null values represents \"no value\" or \"nothing\", it's not even an empty string or zero. It can be used to represent that nothing useful exists. \n",
    "2. NaN stands for \"Not a Number\", it's usually the result of a mathematical operation that doesn't make sense, e.g. 0.0/0.0 \n",
    "\n",
    "Let's cover examples of each of these methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"nulls\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "#Some csv data\n",
    "\n",
    "datasets = \"/home/iron/Documents/1.Learning/3. DF_ESSENTIALS/uw-madison-courses/\"\n",
    "zomato = spark.read.csv(datasets+'zomato.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "This dataset contains the aggregate rating of restaurant in Bengaluru India from Zomato. \n",
    "\n",
    "**Source:** https://www.kaggle.com/himanshupoddar/zomato-bangalore-restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zomato.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate, votes, approx_cost\n",
    "# Edit some var types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = zomato.withColumn(\"approx_cost(for two people)\", zomato[\"approx_cost(for two people)\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"votes\", zomato[\"votes\"].cast(IntegerType()))\n",
    "#QA\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nulls values appear as \"None\" in the Pandas print out above. If we show the null values for the cuisines variable in attempt to view that first restaurant \"Jalsa\", we can see it appear as \"null\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# zomato.filter(\"cuisines='None'\").agg(F.count(zomato.name)).show()\n",
    "df.filter(df.cuisines.isNull()).select(['name','cuisines']).show(5)\n",
    "\n",
    "null_count = nullRows = df.where(col('votes').isNull()).count()\n",
    "\n",
    "print(\"votes has null count of :\", (null_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Statistics\n",
    "\n",
    "It is always valualuable to know how much missing data you are going to be working with before you take any action like filling missing values with an average or dropping rows completly. Here is a good script to get you started. We will also explore more later on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            print(type(temp))\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----------------+---------------------+\n",
      "|Column_Name                |Null_values_count|Null_value_percentage|\n",
      "+---------------------------+-----------------+---------------------+\n",
      "|name                       |85               |0.11849993029415865  |\n",
      "|online_order               |8111             |11.307681583716715   |\n",
      "|book_table                 |2                |0.0027882336539802035|\n",
      "|rate                       |7775             |10.839258329848041   |\n",
      "|votes                      |20018            |27.907430642687856   |\n",
      "|phone                      |1227             |1.7105813467168547   |\n",
      "|location                   |20054            |27.957618848459504   |\n",
      "|rest_type                  |20165            |28.1123658162554     |\n",
      "|dish_liked                 |46841            |65.30182629304335    |\n",
      "|cuisines                   |27305            |38.06635996096473    |\n",
      "|approx_cost(for two people)|43611            |60.798828941865324   |\n",
      "|reviews_list               |28185            |39.293182768716015   |\n",
      "|menu_item                  |28611            |39.8870765370138     |\n",
      "|listed_in(type)            |28983            |40.40568799665412    |\n",
      "|listed_in(city)            |29344            |40.908964171197546   |\n",
      "+---------------------------+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "def null_cols_count(df):\n",
    "    null_count_list = []\n",
    "    numRows = df.count()\n",
    "    for colmn in df.columns:\n",
    "        nullRows = df.where(col(colmn).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = colmn, nullRows, (nullRows/numRows)*100\n",
    "            null_count_list.append(temp)\n",
    "    return null_count_list;\n",
    "\n",
    "null_columns_count_list = null_cols_count(df)\n",
    "spark.createDataFrame(null_columns_count_list, ['Column_Name','Null_values_count','Null_value_percentage']).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "|url|address|name|online_order|book_table|rate|votes|phone|location|rest_type|dish_liked|cuisines|approx_cost(for two people)|reviews_list|menu_item|listed_in(type)|listed_in(city)|\n",
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "|  0|      0|  85|        8111|         2|7775|20018| 1227|   20054|    20165|     46841|   27305|                      43611|       28185|    28611|          28983|          29344|\n",
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "|url|address|name|online_order|book_table|rate|votes|phone|location|rest_type|dish_liked|cuisines|approx_cost(for two people)|reviews_list|menu_item|listed_in(type)|listed_in(city)|\n",
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "|0.0|    0.0| 0.1|        11.3|       0.0|10.8| 27.9|  1.7|    28.0|     28.1|      65.3|    38.1|                       60.8|        39.3|     39.9|           40.4|           40.9|\n",
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "|url|address|name|online_order|book_table|rate|votes|phone|location|rest_type|dish_liked|cuisines|approx_cost(for two people)|reviews_list|menu_item|listed_in(type)|listed_in(city)|\n",
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "|  0|      0|  85|        8111|         2|7775|20018| 1227|   20054|    20165|     46841|   27305|                      43611|       28185|    28611|          28983|          29344|\n",
      "|0.0|    0.0| 0.1|        11.3|       0.0|10.8| 27.9|  1.7|    28.0|     28.1|      65.3|    38.1|                       60.8|        39.3|     39.9|           40.4|           40.9|\n",
      "+---+-------+----+------------+----------+----+-----+-----+--------+---------+----------+--------+---------------------------+------------+---------+---------------+---------------+\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Another way if you prefer\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "#first row: null count\n",
    "nulls = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "print(nulls.count())\n",
    "nulls.show()\n",
    "# Second row: null percent\n",
    "percent = df.select([format_number(((count(when(isnan(c) | col(c).isNull(), c))/df.count())*100),1).alias(c) for c in df.columns])\n",
    "print(percent.count())\n",
    "percent.show()\n",
    "\n",
    "result = nulls.union(percent)\n",
    "result.show()\n",
    "print(result.count())\n",
    "# result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the missing data\n",
    "\n",
    "PySpark has a really handy .na function for working with missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.zomato.com/bangalore/spice-elephan...</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>787</td>\n",
       "      <td>080 41714161</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Momos, Lunch Buffet, Chocolate Nirvana, Thai G...</td>\n",
       "      <td>Chinese, North Indian, Thai</td>\n",
       "      <td>800</td>\n",
       "      <td>\"[('Rated 4.0', 'RATED\\n  Had been here for di...</td>\n",
       "      <td>rice was well cooked and overall was great\\n\\n...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  This place just cool ? with good am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.zomato.com/SanchurroBangalore?cont...</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3.8/5</td>\n",
       "      <td>918</td>\n",
       "      <td>+91 9663487993</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Cafe, Casual Dining</td>\n",
       "      <td>Churros, Cannelloni, Minestrone Soup, Hot Choc...</td>\n",
       "      <td>Cafe, Mexican, Italian</td>\n",
       "      <td>800</td>\n",
       "      <td>\"[('Rated 3.0', \"\"RATED\\n  Ambience is not tha...</td>\n",
       "      <td>('Rated 3.0'</td>\n",
       "      <td>\"\"RATED\\n \\nWent there for a quick bite with ...</td>\n",
       "      <td>pasta churros and lasagne.\\n\\nNachos were pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.zomato.com/bangalore/addhuri-udupi...</td>\n",
       "      <td>1st Floor, Annakuteera, 3rd Stage, Banashankar...</td>\n",
       "      <td>Addhuri Udupi Bhojana</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3.7/5</td>\n",
       "      <td>88</td>\n",
       "      <td>+91 9620009302</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Quick Bites</td>\n",
       "      <td>Masala Dosa</td>\n",
       "      <td>South Indian, North Indian</td>\n",
       "      <td>300</td>\n",
       "      <td>\"[('Rated 4.0', \"\"RATED\\n  Great food and prop...</td>\n",
       "      <td>('Rated 2.0'</td>\n",
       "      <td>'RATED\\n  Reached the place at 3pm on Saturda...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.zomato.com/bangalore/cafe-shuffle-...</td>\n",
       "      <td>941, 3rd FLOOR, 21st Main, 22nd Cross, Banasha...</td>\n",
       "      <td>Cafe Shuffle</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.2/5</td>\n",
       "      <td>150</td>\n",
       "      <td>+91 9742166777</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Cafe</td>\n",
       "      <td>Mocktails, Peri Fries, Lasagne, Pizza, Chicken...</td>\n",
       "      <td>Cafe, Italian, Continental</td>\n",
       "      <td>600</td>\n",
       "      <td>\"[('Rated 1.0', \"\"RATED\\n \\n\\nHorrible. Not ev...</td>\n",
       "      <td>you get it literally half an hour late.\"\")</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>\"\"RATED\\n  While this place is more common fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/spice-elephan...   \n",
       "1  https://www.zomato.com/SanchurroBangalore?cont...   \n",
       "2  https://www.zomato.com/bangalore/addhuri-udupi...   \n",
       "3  https://www.zomato.com/bangalore/cafe-shuffle-...   \n",
       "\n",
       "                                             address                   name  \\\n",
       "0  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...         Spice Elephant   \n",
       "1  1112, Next to KIMS Medical College, 17th Cross...        San Churro Cafe   \n",
       "2  1st Floor, Annakuteera, 3rd Stage, Banashankar...  Addhuri Udupi Bhojana   \n",
       "3  941, 3rd FLOOR, 21st Main, 22nd Cross, Banasha...           Cafe Shuffle   \n",
       "\n",
       "  online_order book_table   rate  votes           phone      location  \\\n",
       "0          Yes         No  4.1/5    787    080 41714161  Banashankari   \n",
       "1          Yes         No  3.8/5    918  +91 9663487993  Banashankari   \n",
       "2           No         No  3.7/5     88  +91 9620009302  Banashankari   \n",
       "3          Yes        Yes  4.2/5    150  +91 9742166777  Banashankari   \n",
       "\n",
       "             rest_type                                         dish_liked  \\\n",
       "0        Casual Dining  Momos, Lunch Buffet, Chocolate Nirvana, Thai G...   \n",
       "1  Cafe, Casual Dining  Churros, Cannelloni, Minestrone Soup, Hot Choc...   \n",
       "2          Quick Bites                                        Masala Dosa   \n",
       "3                 Cafe  Mocktails, Peri Fries, Lasagne, Pizza, Chicken...   \n",
       "\n",
       "                      cuisines  approx_cost(for two people)  \\\n",
       "0  Chinese, North Indian, Thai                          800   \n",
       "1       Cafe, Mexican, Italian                          800   \n",
       "2   South Indian, North Indian                          300   \n",
       "3   Cafe, Italian, Continental                          600   \n",
       "\n",
       "                                        reviews_list  \\\n",
       "0  \"[('Rated 4.0', 'RATED\\n  Had been here for di...   \n",
       "1  \"[('Rated 3.0', \"\"RATED\\n  Ambience is not tha...   \n",
       "2  \"[('Rated 4.0', \"\"RATED\\n  Great food and prop...   \n",
       "3  \"[('Rated 1.0', \"\"RATED\\n \\n\\nHorrible. Not ev...   \n",
       "\n",
       "                                           menu_item  \\\n",
       "0  rice was well cooked and overall was great\\n\\n...   \n",
       "1                                       ('Rated 3.0'   \n",
       "2                                       ('Rated 2.0'   \n",
       "3         you get it literally half an hour late.\"\")   \n",
       "\n",
       "                                     listed_in(type)  \\\n",
       "0                                       ('Rated 5.0'   \n",
       "1   \"\"RATED\\n \\nWent there for a quick bite with ...   \n",
       "2   'RATED\\n  Reached the place at 3pm on Saturda...   \n",
       "3                                       ('Rated 4.0'   \n",
       "\n",
       "                                     listed_in(city)  \n",
       "0   'RATED\\n  This place just cool ? with good am...  \n",
       "1   pasta churros and lasagne.\\n\\nNachos were pat...  \n",
       "2                                       ('Rated 4.0'  \n",
       "3   \"\"RATED\\n  While this place is more common fo...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop any row that contains missing data across the whole dataset\n",
    "df.na.drop().limit(4).toPandas() \n",
    "\n",
    "# Note this statement is equivilant to the above:\n",
    "# df.na.drop(how='any').limit(4).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 278:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 63124\n",
      "Percentage of Rows Dropped 0.8800223058692318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Of course you will want to know how many rows that affected before you actually execute it..\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! 88% is a lot! We better figure out a better method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 282:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Dropped: 1694\n",
      "Percentage of Rows Dropped 0.023616339049212325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop rows that have at least 8 NON-null values\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(thresh=8).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only drop the rows whose values in the sales column are null\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(subset=[\"votes\"]).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do the above\n",
    "og_len = df.count()\n",
    "drop_len = df.filter(df.rate.isNotNull()).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL its values are null.\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(how='all').count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (character value)\n",
    "df.na.fill('MISSING').limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (numeric value)\n",
    "df.na.fill(999).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.isNull()).na.fill('No Name',subset=['name']).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the **mean value** for the column. Here is a fun function to that in an automatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(avg(c).alias(c) for c in df.columns if c in include))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "updated_df = fill_with_mean(df, [\"votes\"])\n",
    "updated_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the missing data\n",
    "A few machine learning algorithms can easily deal with missing data. Just do your research and make sure the nulls values are not impacting the integrity of your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we need to know for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
